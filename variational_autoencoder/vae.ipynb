{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import Input, Flatten, Dense, Conv2D, Conv2DTranspose, Dropout, BatchNormalization, LeakyReLU, Reshape, Activation, Lambda\n",
        "from keras.models import Model\n",
        "from keras import backend as K\n",
        "from keras.optimizers import Adam \n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt \n",
        "import tensorflow as tf\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "from typing import Tuple, List\n",
        "from functools import partial\n",
        "\n",
        "import tensorflow_datasets as tfds"
      ],
      "metadata": {
        "id": "YBR2GtrI7lmR"
      },
      "execution_count": 367,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VariationalAutoEncoder():\n",
        "\n",
        "  def __init__(self, \n",
        "               input_dim: Tuple[int],\n",
        "               encoder_conv_filters: List[int],\n",
        "               encoder_conv_kernel_size: List[int],\n",
        "               encoder_conv_strides: List[int],\n",
        "               decoder_conv_t_filters: List[int],\n",
        "               decoder_conv_t_kernel_size: List[int],\n",
        "               decoder_conv_t_strides: List[int],\n",
        "               z_dim: int,\n",
        "               use_batch_norm=False,\n",
        "               use_dropout=False):\n",
        "\n",
        "    self.name = \"variational_autoencoder\"\n",
        "    self.input_dim = input_dim\n",
        "    self.encoder_conv_filters = encoder_conv_filters\n",
        "    self.encoder_conv_kernel_size = encoder_conv_kernel_size\n",
        "    self.encoder_conv_strides = encoder_conv_strides\n",
        "    self.decoder_conv_t_filters = decoder_conv_t_filters\n",
        "    self.decoder_conv_t_kernel_size = decoder_conv_t_kernel_size\n",
        "    self.decoder_conv_t_strides = decoder_conv_t_strides\n",
        "    self.z_dim = z_dim\n",
        "    self.use_batch_norm = use_batch_norm\n",
        "    self.use_dropout = use_dropout\n",
        "    self.n_layers_encoder = len(encoder_conv_filters)\n",
        "    self.n_layers_decoder = len(decoder_conv_t_filters)\n",
        "\n",
        "    self._build()\n",
        "  \n",
        "  def _build(self):\n",
        "    self._build_encoder()\n",
        "    self._build_decoder()\n",
        "    model_input = self.encoder_input\n",
        "    model_output = self.decoder(self.encoder_output)\n",
        "\n",
        "    self.model = Model(model_input, model_output)\n",
        "\n",
        "  def _build_encoder(self):\n",
        "    self.encoder_input = x = Input(shape=self.input_dim, name=\"encoder_input\")\n",
        "\n",
        "    for i in range(self.n_layers_encoder):\n",
        "      conv_layer = Conv2D(filters=self.encoder_conv_filters[i],\n",
        "                          kernel_size=self.encoder_conv_kernel_size[i],\n",
        "                          strides=self.encoder_conv_strides[i],\n",
        "                          padding=\"same\",\n",
        "                          name=f\"encoder_conv_{i}\")\n",
        "\n",
        "      x = conv_layer(x)\n",
        "      x = LeakyReLU(name=f\"leaky_relu_{i}\")(x)\n",
        "\n",
        "      if self.use_batch_norm:\n",
        "        x = BatchNormalization()(x)\n",
        "\n",
        "      if self.use_dropout:\n",
        "        x = Dropout(p=0.25)(x)\n",
        "\n",
        "    self.shape_before_flatten = K.int_shape(x)[1:]\n",
        "    x = Flatten()(x)\n",
        "\n",
        "    # Instead of outputting just two values - output mean and variance for a normal distribution\n",
        "    self.mu = Dense(units=self.z_dim, name=\"mu\")(x)\n",
        "    self.log_var = Dense(units=self.z_dim, name=\"log_variance\")(x)\n",
        "\n",
        "    self.encoder_mu_log_var = Model(self.encoder_input, (self.mu, self.log_var)) # outputs the values of mu and log_var for a given image \n",
        "\n",
        "    # Sample values from every normal distribution (z_dim means, z_dim log_vars -> z_dim points)\n",
        "    def sampling(args):\n",
        "      mu, log_var = args\n",
        "      epsilon = K.random_normal(shape=K.shape(mu), mean=0.0, stddev=1.0)\n",
        "      return mu + K.exp(log_var / 2) * epsilon\n",
        "\n",
        "    # self.encoder_output = Dense(units=self.z_dim, name=\"encoder_output\")(x)\n",
        "    self.encoder_output = Lambda(sampling, name=\"encoder_output\")([self.mu, self.log_var])\n",
        "    self.encoder = Model(self.encoder_input, self.encoder_output)\n",
        "\n",
        "  def _build_decoder(self):\n",
        "    decoder_input = x = Input(shape=(self.z_dim, ), name=\"decoder_input\")\n",
        "    x = Dense(units=np.prod(self.shape_before_flatten), name=\"dense_1\")(x)\n",
        "    x = Reshape(self.shape_before_flatten)(x)\n",
        "    for i in range(self.n_layers_decoder):\n",
        "      conv_layer = Conv2DTranspose(filters=self.decoder_conv_t_filters[i],\n",
        "                          kernel_size=self.decoder_conv_t_kernel_size[i],\n",
        "                          strides=self.decoder_conv_t_strides[i],\n",
        "                          padding=\"same\",\n",
        "                          name=f\"decoder_conv_t_{i}\")\n",
        "      x = conv_layer(x)\n",
        "      if i < self.n_layers_decoder - 1:\n",
        "        x = LeakyReLU()(x)\n",
        "\n",
        "    decoder_output = Activation(\"relu\", name=\"relu_activation\")(x)\n",
        "    self.decoder = Model(decoder_input, decoder_output, name=\"decoder_network\")\n",
        "\n",
        "  def compile(self, optimizer, mse_loss_factor=1):\n",
        "\n",
        "    def mse_loss(y_true, y_pred):\n",
        "      \"\"\"\n",
        "      Reconstruction / Generative loss - Mean Squared Error\n",
        "      \"\"\"\n",
        "      return K.mean(K.square(y_true - y_pred), axis=[1, 2, 3])\n",
        "  \n",
        "    def kl_loss(y_true, y_pred):\n",
        "      \"\"\"\n",
        "      Latent loss\n",
        "      Kullback-Leibler Divergence - measures how much one probability distribution differs from another.\n",
        "      In this case, we're measuring how big is the difference between distributions in latent space and \n",
        "      a standard normal one. \n",
        "      Goal - minimize the distance to make the latent space more dense (points in the latent space will be located close to each other).\n",
        "      \"\"\"\n",
        "      return -0.5 * K.sum(1 + self.log_var - K.square(self.mu) - K.exp(self.log_var), axis=1)\n",
        "\n",
        "    def loss_function(y_true, \n",
        "                      y_pred):\n",
        "      return mse_loss_factor * mse_loss(y_true, y_pred) + kl_loss(y_true, y_pred)\n",
        "\n",
        "    self.model.compile(optimizer=optimizer, loss=loss_function, \n",
        "                       metrics=[mse_loss, kl_loss],\n",
        "                       experimental_run_tf_function=False)\n",
        "\n",
        "  def fit(self, \n",
        "          dataloader, \n",
        "          epochs, \n",
        "          callbacks_list=[]):\n",
        "    history = self.model.fit(dataloader.repeat(epochs), epochs=epochs, steps_per_epoch=60000 // dataloader.element_spec[0].shape[0], callbacks=callbacks_list)\n",
        "    return history "
      ],
      "metadata": {
        "id": "6NsYBA0ErwR5"
      },
      "execution_count": 238,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds_train = tfds.load(\n",
        "    'mnist',\n",
        "    split=['train'],\n",
        "    shuffle_files=True,\n",
        "    as_supervised=False,\n",
        ")\n",
        "\n",
        "ds_train = ds_train[0]"
      ],
      "metadata": {
        "id": "mYZt-y0uyt63"
      },
      "execution_count": 359,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds_train = ds_train.map(lambda x: tf.cast(x[\"image\"], tf.float32) / 255.)\n",
        "ds_train = tf.data.Dataset.zip((ds_train, ds_train))\n",
        "ds_train = ds_train.batch(64, drop_remainder=True).shuffle(1000)"
      ],
      "metadata": {
        "id": "q5J1wl2N-wfR"
      },
      "execution_count": 331,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vae = VariationalAutoEncoder(input_dim=(28, 28, 1),\n",
        "                               encoder_conv_filters=[32, 64, 64, 64],\n",
        "                               encoder_conv_kernel_size=[3, 3, 3, 3],\n",
        "                               encoder_conv_strides=[1, 2, 2, 1],\n",
        "                               decoder_conv_t_filters=[64, 64, 32, 1],\n",
        "                               decoder_conv_t_kernel_size=[3, 3, 3, 3],\n",
        "                               decoder_conv_t_strides=[1, 2, 2, 1],\n",
        "                               z_dim=2)\n",
        "\n",
        "vae.compile(\n",
        "    optimizer=Adam(learning_rate=0.0005),\n",
        "    mse_loss_factor=5000\n",
        ")\n",
        "\n",
        "vae.fit(ds_train, epochs=100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y4cp6ybw8AYf",
        "outputId": "fc7dc10c-c5b6-4f74-edd0-37025231acf1"
      },
      "execution_count": 246,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train on 937 steps\n",
            "Epoch 1/100\n",
            "937/937 [==============================] - 21s 11ms/step - batch: 468.0000 - size: 1.0000 - loss: 273.2305 - mse_loss: 0.0536 - kl_loss: 4.9983\n",
            "Epoch 2/100\n",
            "937/937 [==============================] - 10s 11ms/step - batch: 468.0000 - size: 1.0000 - loss: 237.8856 - mse_loss: 0.0464 - kl_loss: 5.8307\n",
            "Epoch 3/100\n",
            "937/937 [==============================] - 10s 11ms/step - batch: 468.0000 - size: 1.0000 - loss: 229.3753 - mse_loss: 0.0446 - kl_loss: 6.1385\n",
            "Epoch 4/100\n",
            "937/937 [==============================] - 11s 11ms/step - batch: 468.0000 - size: 1.0000 - loss: 224.3430 - mse_loss: 0.0436 - kl_loss: 6.3022\n",
            "Epoch 5/100\n",
            "937/937 [==============================] - 10s 11ms/step - batch: 468.0000 - size: 1.0000 - loss: 220.7129 - mse_loss: 0.0429 - kl_loss: 6.4286\n",
            "Epoch 6/100\n",
            "937/937 [==============================] - 10s 11ms/step - batch: 468.0000 - size: 1.0000 - loss: 217.8855 - mse_loss: 0.0423 - kl_loss: 6.5424\n",
            "Epoch 7/100\n",
            "937/937 [==============================] - 9s 10ms/step - batch: 468.0000 - size: 1.0000 - loss: 215.4717 - mse_loss: 0.0418 - kl_loss: 6.6349\n",
            "Epoch 8/100\n",
            "937/937 [==============================] - 10s 11ms/step - batch: 468.0000 - size: 1.0000 - loss: 213.4261 - mse_loss: 0.0413 - kl_loss: 6.7134\n",
            "Epoch 9/100\n",
            "937/937 [==============================] - 10s 11ms/step - batch: 468.0000 - size: 1.0000 - loss: 211.9606 - mse_loss: 0.0410 - kl_loss: 6.7581\n",
            "Epoch 10/100\n",
            "937/937 [==============================] - 10s 11ms/step - batch: 468.0000 - size: 1.0000 - loss: 210.5411 - mse_loss: 0.0408 - kl_loss: 6.7765\n",
            "Epoch 11/100\n",
            "937/937 [==============================] - 10s 11ms/step - batch: 468.0000 - size: 1.0000 - loss: 209.5189 - mse_loss: 0.0405 - kl_loss: 6.8212\n",
            "Epoch 12/100\n",
            "937/937 [==============================] - 9s 10ms/step - batch: 468.0000 - size: 1.0000 - loss: 208.3534 - mse_loss: 0.0403 - kl_loss: 6.8521\n",
            "Epoch 13/100\n",
            "937/937 [==============================] - 10s 11ms/step - batch: 468.0000 - size: 1.0000 - loss: 207.5903 - mse_loss: 0.0401 - kl_loss: 6.8848\n",
            "Epoch 14/100\n",
            "937/937 [==============================] - 10s 11ms/step - batch: 468.0000 - size: 1.0000 - loss: 206.7010 - mse_loss: 0.0400 - kl_loss: 6.9028\n",
            "Epoch 15/100\n",
            "937/937 [==============================] - 10s 11ms/step - batch: 468.0000 - size: 1.0000 - loss: 206.1743 - mse_loss: 0.0398 - kl_loss: 6.9442\n",
            "Epoch 16/100\n",
            "937/937 [==============================] - 10s 11ms/step - batch: 468.0000 - size: 1.0000 - loss: 205.4599 - mse_loss: 0.0397 - kl_loss: 6.9517\n",
            "Epoch 17/100\n",
            "937/937 [==============================] - 9s 10ms/step - batch: 468.0000 - size: 1.0000 - loss: 204.7664 - mse_loss: 0.0396 - kl_loss: 6.9722\n",
            "Epoch 18/100\n",
            "937/937 [==============================] - 10s 11ms/step - batch: 468.0000 - size: 1.0000 - loss: 204.1072 - mse_loss: 0.0394 - kl_loss: 7.0021\n",
            "Epoch 19/100\n",
            "937/937 [==============================] - 10s 11ms/step - batch: 468.0000 - size: 1.0000 - loss: 203.7632 - mse_loss: 0.0393 - kl_loss: 7.0143\n",
            "Epoch 20/100\n",
            "937/937 [==============================] - 10s 11ms/step - batch: 468.0000 - size: 1.0000 - loss: 203.0822 - mse_loss: 0.0392 - kl_loss: 6.9986\n",
            "Epoch 21/100\n",
            "937/937 [==============================] - 9s 10ms/step - batch: 468.0000 - size: 1.0000 - loss: 202.6201 - mse_loss: 0.0391 - kl_loss: 7.0374\n",
            "Epoch 22/100\n",
            "937/937 [==============================] - 10s 10ms/step - batch: 468.0000 - size: 1.0000 - loss: 202.1970 - mse_loss: 0.0390 - kl_loss: 7.0529\n",
            "Epoch 23/100\n",
            "937/937 [==============================] - 10s 10ms/step - batch: 468.0000 - size: 1.0000 - loss: 201.9023 - mse_loss: 0.0390 - kl_loss: 7.0869\n",
            "Epoch 24/100\n",
            "937/937 [==============================] - 10s 10ms/step - batch: 468.0000 - size: 1.0000 - loss: 201.4720 - mse_loss: 0.0389 - kl_loss: 7.0994\n",
            "Epoch 25/100\n",
            "937/937 [==============================] - 9s 10ms/step - batch: 468.0000 - size: 1.0000 - loss: 201.0405 - mse_loss: 0.0388 - kl_loss: 7.0867\n",
            "Epoch 26/100\n",
            "937/937 [==============================] - 9s 10ms/step - batch: 468.0000 - size: 1.0000 - loss: 200.7854 - mse_loss: 0.0387 - kl_loss: 7.1027\n",
            "Epoch 27/100\n",
            "937/937 [==============================] - 10s 11ms/step - batch: 468.0000 - size: 1.0000 - loss: 200.2583 - mse_loss: 0.0386 - kl_loss: 7.1024\n",
            "Epoch 28/100\n",
            "937/937 [==============================] - 10s 11ms/step - batch: 468.0000 - size: 1.0000 - loss: 199.8963 - mse_loss: 0.0386 - kl_loss: 7.1257\n",
            "Epoch 29/100\n",
            "937/937 [==============================] - 10s 11ms/step - batch: 468.0000 - size: 1.0000 - loss: 199.7393 - mse_loss: 0.0385 - kl_loss: 7.1321\n",
            "Epoch 30/100\n",
            "937/937 [==============================] - 10s 10ms/step - batch: 468.0000 - size: 1.0000 - loss: 199.3900 - mse_loss: 0.0385 - kl_loss: 7.1141\n",
            "Epoch 31/100\n",
            "937/937 [==============================] - 10s 10ms/step - batch: 468.0000 - size: 1.0000 - loss: 199.0142 - mse_loss: 0.0384 - kl_loss: 7.1642\n",
            "Epoch 32/100\n",
            "937/937 [==============================] - 10s 10ms/step - batch: 468.0000 - size: 1.0000 - loss: 198.7241 - mse_loss: 0.0383 - kl_loss: 7.1628\n",
            "Epoch 33/100\n",
            "937/937 [==============================] - 10s 10ms/step - batch: 468.0000 - size: 1.0000 - loss: 198.5866 - mse_loss: 0.0383 - kl_loss: 7.1802\n",
            "Epoch 34/100\n",
            "937/937 [==============================] - 10s 11ms/step - batch: 468.0000 - size: 1.0000 - loss: 198.4420 - mse_loss: 0.0383 - kl_loss: 7.1665\n",
            "Epoch 35/100\n",
            "937/937 [==============================] - 9s 10ms/step - batch: 468.0000 - size: 1.0000 - loss: 198.0995 - mse_loss: 0.0382 - kl_loss: 7.1759\n",
            "Epoch 36/100\n",
            "937/937 [==============================] - 10s 11ms/step - batch: 468.0000 - size: 1.0000 - loss: 197.6982 - mse_loss: 0.0381 - kl_loss: 7.1938\n",
            "Epoch 37/100\n",
            "937/937 [==============================] - 10s 11ms/step - batch: 468.0000 - size: 1.0000 - loss: 197.2834 - mse_loss: 0.0380 - kl_loss: 7.1930\n",
            "Epoch 38/100\n",
            "937/937 [==============================] - 10s 10ms/step - batch: 468.0000 - size: 1.0000 - loss: 197.1141 - mse_loss: 0.0380 - kl_loss: 7.2044\n",
            "Epoch 39/100\n",
            "937/937 [==============================] - 9s 10ms/step - batch: 468.0000 - size: 1.0000 - loss: 197.1508 - mse_loss: 0.0380 - kl_loss: 7.1992\n",
            "Epoch 40/100\n",
            "937/937 [==============================] - 10s 10ms/step - batch: 468.0000 - size: 1.0000 - loss: 196.8744 - mse_loss: 0.0379 - kl_loss: 7.2017\n",
            "Epoch 41/100\n",
            "937/937 [==============================] - 10s 10ms/step - batch: 468.0000 - size: 1.0000 - loss: 196.6276 - mse_loss: 0.0379 - kl_loss: 7.2278\n",
            "Epoch 42/100\n",
            "937/937 [==============================] - 10s 11ms/step - batch: 468.0000 - size: 1.0000 - loss: 196.4186 - mse_loss: 0.0378 - kl_loss: 7.2648\n",
            "Epoch 43/100\n",
            "937/937 [==============================] - 10s 10ms/step - batch: 468.0000 - size: 1.0000 - loss: 196.2994 - mse_loss: 0.0378 - kl_loss: 7.2330\n",
            "Epoch 44/100\n",
            "937/937 [==============================] - 10s 11ms/step - batch: 468.0000 - size: 1.0000 - loss: 196.0662 - mse_loss: 0.0378 - kl_loss: 7.2648\n",
            "Epoch 45/100\n",
            "937/937 [==============================] - 10s 11ms/step - batch: 468.0000 - size: 1.0000 - loss: 195.8264 - mse_loss: 0.0377 - kl_loss: 7.2511\n",
            "Epoch 46/100\n",
            "937/937 [==============================] - 10s 11ms/step - batch: 468.0000 - size: 1.0000 - loss: 195.6994 - mse_loss: 0.0377 - kl_loss: 7.2500\n",
            "Epoch 47/100\n",
            "937/937 [==============================] - 10s 10ms/step - batch: 468.0000 - size: 1.0000 - loss: 195.5408 - mse_loss: 0.0377 - kl_loss: 7.2613\n",
            "Epoch 48/100\n",
            "937/937 [==============================] - 9s 10ms/step - batch: 468.0000 - size: 1.0000 - loss: 195.2729 - mse_loss: 0.0376 - kl_loss: 7.2597\n",
            "Epoch 49/100\n",
            "937/937 [==============================] - 9s 10ms/step - batch: 468.0000 - size: 1.0000 - loss: 195.0352 - mse_loss: 0.0376 - kl_loss: 7.2622\n",
            "Epoch 50/100\n",
            "937/937 [==============================] - 10s 10ms/step - batch: 468.0000 - size: 1.0000 - loss: 194.9120 - mse_loss: 0.0375 - kl_loss: 7.2984\n",
            "Epoch 51/100\n",
            "937/937 [==============================] - 10s 10ms/step - batch: 468.0000 - size: 1.0000 - loss: 194.8749 - mse_loss: 0.0375 - kl_loss: 7.2999\n",
            "Epoch 52/100\n",
            "937/937 [==============================] - 10s 10ms/step - batch: 468.0000 - size: 1.0000 - loss: 194.5753 - mse_loss: 0.0375 - kl_loss: 7.3039\n",
            "Epoch 53/100\n",
            "937/937 [==============================] - 9s 10ms/step - batch: 468.0000 - size: 1.0000 - loss: 194.5711 - mse_loss: 0.0375 - kl_loss: 7.2928\n",
            "Epoch 54/100\n",
            "937/937 [==============================] - 10s 10ms/step - batch: 468.0000 - size: 1.0000 - loss: 194.3306 - mse_loss: 0.0374 - kl_loss: 7.2957\n",
            "Epoch 55/100\n",
            "937/937 [==============================] - 10s 10ms/step - batch: 468.0000 - size: 1.0000 - loss: 194.2549 - mse_loss: 0.0374 - kl_loss: 7.2943\n",
            "Epoch 56/100\n",
            "937/937 [==============================] - 10s 10ms/step - batch: 468.0000 - size: 1.0000 - loss: 193.8954 - mse_loss: 0.0373 - kl_loss: 7.3152\n",
            "Epoch 57/100\n",
            "937/937 [==============================] - 9s 10ms/step - batch: 468.0000 - size: 1.0000 - loss: 193.7418 - mse_loss: 0.0373 - kl_loss: 7.3027\n",
            "Epoch 58/100\n",
            "937/937 [==============================] - 10s 11ms/step - batch: 468.0000 - size: 1.0000 - loss: 193.9009 - mse_loss: 0.0373 - kl_loss: 7.3237\n",
            "Epoch 59/100\n",
            "937/937 [==============================] - 10s 11ms/step - batch: 468.0000 - size: 1.0000 - loss: 193.7287 - mse_loss: 0.0373 - kl_loss: 7.3395\n",
            "Epoch 60/100\n",
            "937/937 [==============================] - 10s 11ms/step - batch: 468.0000 - size: 1.0000 - loss: 193.5289 - mse_loss: 0.0372 - kl_loss: 7.3315\n",
            "Epoch 61/100\n",
            "937/937 [==============================] - 10s 10ms/step - batch: 468.0000 - size: 1.0000 - loss: 193.4307 - mse_loss: 0.0372 - kl_loss: 7.3490\n",
            "Epoch 62/100\n",
            "937/937 [==============================] - 9s 10ms/step - batch: 468.0000 - size: 1.0000 - loss: 193.0456 - mse_loss: 0.0371 - kl_loss: 7.3517\n",
            "Epoch 63/100\n",
            "937/937 [==============================] - 10s 11ms/step - batch: 468.0000 - size: 1.0000 - loss: 193.1034 - mse_loss: 0.0372 - kl_loss: 7.3274\n",
            "Epoch 64/100\n",
            "937/937 [==============================] - 10s 11ms/step - batch: 468.0000 - size: 1.0000 - loss: 193.0402 - mse_loss: 0.0371 - kl_loss: 7.3294\n",
            "Epoch 65/100\n",
            "937/937 [==============================] - 10s 11ms/step - batch: 468.0000 - size: 1.0000 - loss: 192.8530 - mse_loss: 0.0371 - kl_loss: 7.3615\n",
            "Epoch 66/100\n",
            "937/937 [==============================] - 9s 10ms/step - batch: 468.0000 - size: 1.0000 - loss: 192.7614 - mse_loss: 0.0371 - kl_loss: 7.3545\n",
            "Epoch 67/100\n",
            "937/937 [==============================] - 10s 11ms/step - batch: 468.0000 - size: 1.0000 - loss: 192.7075 - mse_loss: 0.0371 - kl_loss: 7.3537\n",
            "Epoch 68/100\n",
            "937/937 [==============================] - 10s 11ms/step - batch: 468.0000 - size: 1.0000 - loss: 192.3970 - mse_loss: 0.0370 - kl_loss: 7.3643\n",
            "Epoch 69/100\n",
            "937/937 [==============================] - 10s 11ms/step - batch: 468.0000 - size: 1.0000 - loss: 192.4466 - mse_loss: 0.0370 - kl_loss: 7.3756\n",
            "Epoch 70/100\n",
            "937/937 [==============================] - 10s 10ms/step - batch: 468.0000 - size: 1.0000 - loss: 192.2812 - mse_loss: 0.0370 - kl_loss: 7.3738\n",
            "Epoch 71/100\n",
            "937/937 [==============================] - 9s 10ms/step - batch: 468.0000 - size: 1.0000 - loss: 192.0273 - mse_loss: 0.0369 - kl_loss: 7.3748\n",
            "Epoch 72/100\n",
            "937/937 [==============================] - 10s 11ms/step - batch: 468.0000 - size: 1.0000 - loss: 192.0153 - mse_loss: 0.0369 - kl_loss: 7.3695\n",
            "Epoch 73/100\n",
            "937/937 [==============================] - 10s 11ms/step - batch: 468.0000 - size: 1.0000 - loss: 191.8929 - mse_loss: 0.0369 - kl_loss: 7.4087\n",
            "Epoch 74/100\n",
            "937/937 [==============================] - 10s 11ms/step - batch: 468.0000 - size: 1.0000 - loss: 191.9431 - mse_loss: 0.0369 - kl_loss: 7.4019\n",
            "Epoch 75/100\n",
            "937/937 [==============================] - 10s 10ms/step - batch: 468.0000 - size: 1.0000 - loss: 191.6650 - mse_loss: 0.0369 - kl_loss: 7.3997\n",
            "Epoch 76/100\n",
            "937/937 [==============================] - 10s 10ms/step - batch: 468.0000 - size: 1.0000 - loss: 191.5797 - mse_loss: 0.0368 - kl_loss: 7.3959\n",
            "Epoch 77/100\n",
            "937/937 [==============================] - 10s 11ms/step - batch: 468.0000 - size: 1.0000 - loss: 191.6580 - mse_loss: 0.0369 - kl_loss: 7.4067\n",
            "Epoch 78/100\n",
            "937/937 [==============================] - 10s 11ms/step - batch: 468.0000 - size: 1.0000 - loss: 191.3850 - mse_loss: 0.0368 - kl_loss: 7.4115\n",
            "Epoch 79/100\n",
            "937/937 [==============================] - 10s 11ms/step - batch: 468.0000 - size: 1.0000 - loss: 191.3276 - mse_loss: 0.0368 - kl_loss: 7.4023\n",
            "Epoch 80/100\n",
            "937/937 [==============================] - 9s 10ms/step - batch: 468.0000 - size: 1.0000 - loss: 191.2042 - mse_loss: 0.0368 - kl_loss: 7.4033\n",
            "Epoch 81/100\n",
            "937/937 [==============================] - 10s 10ms/step - batch: 468.0000 - size: 1.0000 - loss: 191.2621 - mse_loss: 0.0368 - kl_loss: 7.4098\n",
            "Epoch 82/100\n",
            "937/937 [==============================] - 10s 11ms/step - batch: 468.0000 - size: 1.0000 - loss: 190.9429 - mse_loss: 0.0367 - kl_loss: 7.4282\n",
            "Epoch 83/100\n",
            "937/937 [==============================] - 10s 11ms/step - batch: 468.0000 - size: 1.0000 - loss: 190.9949 - mse_loss: 0.0367 - kl_loss: 7.4211\n",
            "Epoch 84/100\n",
            "937/937 [==============================] - 9s 10ms/step - batch: 468.0000 - size: 1.0000 - loss: 190.8272 - mse_loss: 0.0367 - kl_loss: 7.4079\n",
            "Epoch 85/100\n",
            "937/937 [==============================] - 10s 10ms/step - batch: 468.0000 - size: 1.0000 - loss: 190.7874 - mse_loss: 0.0367 - kl_loss: 7.4166\n",
            "Epoch 86/100\n",
            "937/937 [==============================] - 10s 10ms/step - batch: 468.0000 - size: 1.0000 - loss: 190.7553 - mse_loss: 0.0367 - kl_loss: 7.4195\n",
            "Epoch 87/100\n",
            "937/937 [==============================] - 10s 11ms/step - batch: 468.0000 - size: 1.0000 - loss: 190.5841 - mse_loss: 0.0366 - kl_loss: 7.4240\n",
            "Epoch 88/100\n",
            "937/937 [==============================] - 9s 10ms/step - batch: 468.0000 - size: 1.0000 - loss: 190.5379 - mse_loss: 0.0366 - kl_loss: 7.4447\n",
            "Epoch 89/100\n",
            "937/937 [==============================] - 9s 10ms/step - batch: 468.0000 - size: 1.0000 - loss: 190.4063 - mse_loss: 0.0366 - kl_loss: 7.4436\n",
            "Epoch 90/100\n",
            "937/937 [==============================] - 10s 11ms/step - batch: 468.0000 - size: 1.0000 - loss: 190.4984 - mse_loss: 0.0366 - kl_loss: 7.4123\n",
            "Epoch 91/100\n",
            "937/937 [==============================] - 10s 10ms/step - batch: 468.0000 - size: 1.0000 - loss: 190.3261 - mse_loss: 0.0366 - kl_loss: 7.4636\n",
            "Epoch 92/100\n",
            "937/937 [==============================] - 10s 10ms/step - batch: 468.0000 - size: 1.0000 - loss: 190.1358 - mse_loss: 0.0365 - kl_loss: 7.4515\n",
            "Epoch 93/100\n",
            "937/937 [==============================] - 9s 10ms/step - batch: 468.0000 - size: 1.0000 - loss: 190.2055 - mse_loss: 0.0366 - kl_loss: 7.4408\n",
            "Epoch 94/100\n",
            "937/937 [==============================] - 10s 11ms/step - batch: 468.0000 - size: 1.0000 - loss: 189.9964 - mse_loss: 0.0365 - kl_loss: 7.4372\n",
            "Epoch 95/100\n",
            "937/937 [==============================] - 10s 11ms/step - batch: 468.0000 - size: 1.0000 - loss: 189.9852 - mse_loss: 0.0365 - kl_loss: 7.4572\n",
            "Epoch 96/100\n",
            "937/937 [==============================] - 10s 11ms/step - batch: 468.0000 - size: 1.0000 - loss: 189.9406 - mse_loss: 0.0365 - kl_loss: 7.4650\n",
            "Epoch 97/100\n",
            "937/937 [==============================] - 9s 10ms/step - batch: 468.0000 - size: 1.0000 - loss: 189.7850 - mse_loss: 0.0365 - kl_loss: 7.4767\n",
            "Epoch 98/100\n",
            "937/937 [==============================] - 10s 11ms/step - batch: 468.0000 - size: 1.0000 - loss: 189.5497 - mse_loss: 0.0364 - kl_loss: 7.4439\n",
            "Epoch 99/100\n",
            "937/937 [==============================] - 9s 10ms/step - batch: 468.0000 - size: 1.0000 - loss: 189.5537 - mse_loss: 0.0364 - kl_loss: 7.4567\n",
            "Epoch 100/100\n",
            "937/937 [==============================] - 7s 7ms/step - batch: 468.0000 - size: 1.0000 - loss: 189.6314 - mse_loss: 0.0364 - kl_loss: 7.4527\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f427f70e880>"
            ]
          },
          "metadata": {},
          "execution_count": 246
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt \n",
        "\n",
        "def decode(z):\n",
        "  res = vae.decoder.predict((tf.constant([z], dtype=tf.float32)), steps=1)\n",
        "  res = res * 255.0\n",
        "  return res.reshape([28, -1])"
      ],
      "metadata": {
        "id": "jzxCN3lpMjpg"
      },
      "execution_count": 292,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = decode([1.1, -0.2])\n",
        "plt.imshow(a)\n",
        "# print(a.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 448
        },
        "id": "zTH3B1i-MuOi",
        "outputId": "afbe1383-3564-4be9-f3e7-c76382c7cd2a"
      },
      "execution_count": 335,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f4270182490>"
            ]
          },
          "metadata": {},
          "execution_count": 335
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sp3fPMu8WC1X"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}